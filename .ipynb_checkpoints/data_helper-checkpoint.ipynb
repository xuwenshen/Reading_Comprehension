{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from tqdm import *\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import random\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import h5py\n",
    "\n",
    "wpt = WordPunctTokenizer()\n",
    "\n",
    "min_context_len = 20\n",
    "max_context_len = 350\n",
    "min_question_len = 2\n",
    "max_question_len = 30\n",
    "max_answer_len = 30\n",
    "\n",
    "def helper(data_path,voc_path, number_data = None):\n",
    "    \n",
    "    data = json.load(open(data_path))\n",
    "    voc = json.load(open('/data/xuwenshen/workspace/squad/data/train_voc.json'))\n",
    "\n",
    "    p_set = []\n",
    "    p_len_set = []\n",
    "    p_c_s_e_set = []\n",
    "\n",
    "    q_set = []\n",
    "    q_len_set = []\n",
    "    g_set = []\n",
    "\n",
    "    a_set = []\n",
    "    a_len_set = []\n",
    "    a_b_set = []\n",
    "\n",
    "    p_str_set = []\n",
    "    q_str_set = []\n",
    "    a_str_set = []\n",
    "\n",
    "    k_set = []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        passage = data[i]['passage']\n",
    "        question = data[i]['question']\n",
    "        answer_text = data[i]['answertext']\n",
    "        answer_start = data[i]['answerstart']\n",
    "        key = data[i]['id']\n",
    "\n",
    "        answer_end = answer_start + len(answer_text) - 1\n",
    "\n",
    "        p_tmp = [voc['pad#'] for j in range(max_context_len+1)]\n",
    "        p_len_tmp = 0\n",
    "        p_c_s_e_tmp = [[-1, -1] for j in range(max_context_len+1)]\n",
    "\n",
    "        a_tmp = [voc['pad#'] for j in range(max_answer_len+1)]\n",
    "        a_len_tmp = 0\n",
    "        a_b_tmp = [1, 0]\n",
    "\n",
    "        start = -1\n",
    "        end = -1\n",
    "\n",
    "        for wi, (cs, ce) in enumerate(wpt.span_tokenize(passage)):\n",
    "\n",
    "            if wi == max_context_len: break\n",
    "                \n",
    "            if cs <= answer_start and ce >= answer_start:\n",
    "                start = wi\n",
    "            if cs <= answer_end and ce >= answer_end:\n",
    "                end = wi\n",
    "\n",
    "            p_c_s_e_tmp[wi] = [cs, ce]\n",
    "\n",
    "            word = passage[cs:ce].lower()\n",
    "            if word in voc:\n",
    "                p_tmp[wi] = voc[word]\n",
    "            else:\n",
    "                p_tmp[wi] = voc['unk#']\n",
    "            p_len_tmp = wi + 1\n",
    "\n",
    "        p_set.append(p_tmp)\n",
    "        p_len_set.append(p_len_tmp)\n",
    "        p_c_s_e_set.append(p_c_s_e_tmp)\n",
    " \n",
    "        # 在dev中，如果没有发现完整的answer，就不写入了\n",
    "        if not (end == -1 or start == -1):\n",
    "            #在dev中，answer太长，就clip\n",
    "            end = min(end+1, start+max_answer_len)\n",
    "            for j in range(start, end+1):\n",
    "                a_tmp[j-start] = p_tmp[j]\n",
    "            a_len_tmp = end-start+1\n",
    "            a_b_tmp = [start, end]\n",
    "\n",
    "        a_set.append(a_tmp)\n",
    "        a_len_set.append(a_len_tmp)\n",
    "        a_b_set.append(a_b_tmp)\n",
    "\n",
    "        q_tmp = [voc['pad#'] for j in range(max_question_len+2)]\n",
    "        q_len_tmp = 0\n",
    "        gtruth_tmp = [voc['pad#'] for j in range(max_question_len + 2)]\n",
    "        for wi, token in enumerate(wpt.tokenize(question)):\n",
    "            \n",
    "            if wi == max_question_len: break\n",
    "            \n",
    "            if token.lower() in voc:\n",
    "                q_tmp[wi] = voc[token.lower()]\n",
    "                gtruth_tmp[wi+1] = voc[token.lower()]\n",
    "            else:\n",
    "                q_tmp[wi] = voc['unk#']\n",
    "                gtruth_tmp[wi+1] = voc['unk#']\n",
    "            q_len_tmp = wi + 1\n",
    "\n",
    "        q_tmp[q_len_tmp] = voc['eos#']\n",
    "        gtruth_tmp[0] = voc['go#']\n",
    "        gtruth_tmp[q_len_tmp] = voc['eos#']\n",
    "        # eos\n",
    "        q_len_tmp += 1\n",
    "\n",
    "        q_set.append(q_tmp)\n",
    "        g_set.append(gtruth_tmp)\n",
    "        q_len_set.append(q_len_tmp)\n",
    "\n",
    "\n",
    "        p_str_set.append(passage)\n",
    "        q_str_set.append(question)\n",
    "        a_str_set.append(answer_text)\n",
    "        k_set.append(key)\n",
    "\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for i in range(len(k_set)):\n",
    "        tmp_dict = defaultdict(lambda : 0)\n",
    "\n",
    "        tmp_dict['passage_tokens'] = np.array(p_set[i])\n",
    "        tmp_dict['passage_len'] = p_len_set[i]\n",
    "        tmp_dict['char_start_end'] = np.array(p_c_s_e_set[i]) \n",
    "        tmp_dict['question_tokens'] = np.array(q_set[i])\n",
    "        tmp_dict['question_len'] = q_len_set[i]\n",
    "        tmp_dict['ground_truth'] = np.array(g_set[i])\n",
    "        tmp_dict['answer_tokens'] = np.array(a_set[i])\n",
    "        tmp_dict['answer_len'] = a_len_set[i]\n",
    "        tmp_dict['boundary'] = np.array(a_b_set[i])\n",
    "        tmp_dict['passage_str'] = p_str_set[i]\n",
    "        tmp_dict['question_str'] = q_str_set[i]\n",
    "        tmp_dict['answer_str'] = a_str_set[i]\n",
    "        tmp_dict['key'] = k_set[i]\n",
    "        \n",
    "        tmp_dict = dict(tmp_dict)\n",
    "        data.append(tmp_dict)\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    if number_data != None:\n",
    "        return data[:number_data]\n",
    "    return data\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
